{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6785, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Jun 03 20:59:36 +0000 2019</td>\n",
       "      <td>barackobama</td>\n",
       "      <td>2873</td>\n",
       "      <td>What a life. American history has always been ...</td>\n",
       "      <td>1135652276515823617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu May 30 20:12:33 +0000 2019</td>\n",
       "      <td>barackobama</td>\n",
       "      <td>13781</td>\n",
       "      <td>A story worth sharing: Twelve years ago, Brian...</td>\n",
       "      <td>1134190885036535809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu May 30 15:17:03 +0000 2019</td>\n",
       "      <td>barackobama</td>\n",
       "      <td>4745</td>\n",
       "      <td>Exciting to see the faces of Colombia's future...</td>\n",
       "      <td>1134116519066165249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon May 27 15:26:55 +0000 2019</td>\n",
       "      <td>barackobama</td>\n",
       "      <td>43244</td>\n",
       "      <td>On Memorial Day, we remember all those who gav...</td>\n",
       "      <td>1133031835880099840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu May 23 14:07:16 +0000 2019</td>\n",
       "      <td>barackobama</td>\n",
       "      <td>15144</td>\n",
       "      <td>Great to get out there and take a few cuts at ...</td>\n",
       "      <td>1131562241478864896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at       handle  retweet_count  \\\n",
       "0  Mon Jun 03 20:59:36 +0000 2019  barackobama           2873   \n",
       "1  Thu May 30 20:12:33 +0000 2019  barackobama          13781   \n",
       "2  Thu May 30 15:17:03 +0000 2019  barackobama           4745   \n",
       "3  Mon May 27 15:26:55 +0000 2019  barackobama          43244   \n",
       "4  Thu May 23 14:07:16 +0000 2019  barackobama          15144   \n",
       "\n",
       "                                                text                   id  \n",
       "0  What a life. American history has always been ...  1135652276515823617  \n",
       "1  A story worth sharing: Twelve years ago, Brian...  1134190885036535809  \n",
       "2  Exciting to see the faces of Colombia's future...  1134116519066165249  \n",
       "3  On Memorial Day, we remember all those who gav...  1133031835880099840  \n",
       "4  Great to get out there and take a few cuts at ...  1131562241478864896  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "basepath = 'C:/Users/Kelsey/Desktop/jobs/TheTrevorProject/Project/data/'\n",
    "bdata = pd.read_csv(basepath + 'barackobama.csv')\n",
    "cdata = pd.read_csv(basepath + 'calvinstowell.csv')\n",
    "kdata = pd.read_csv(basepath + 'kimkardashian.csv')\n",
    "\n",
    "# Combine data\n",
    "alldata = pd.concat([bdata, cdata, kdata])\n",
    "print(alldata.shape)\n",
    "alldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 5)\n",
      "(600, 5)\n",
      "(3000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Split dataframes\n",
    "# Feel like i should sample larger classes & test, but maybe not to Obama size since he is gonna be the easiest to detect?\n",
    "# 1 - undersample calvin & kim's and do split on 1000 tweet corpuses\n",
    "# 2 - cross validation on samples & test\n",
    "# 3 - Run 2 class classification models w/ calvin & kim only and see if helps differentiate\n",
    "# 4? - Markov chain obama and create more tweets, hah\n",
    "\n",
    "## Option 1 - 80/20 of 1000 tweet samples\n",
    "# Limit Calvin & Kim to random 1000\n",
    "cdata_lim = cdata.sample(n=1000, random_state=1)\n",
    "cdata_left = cdata.drop(cdata_lim.index)\n",
    "kdata_lim = kdata.sample(n=1000, random_state=1)\n",
    "kdata_left = kdata.drop(kdata_lim.index)\n",
    "\n",
    "# Split & Combine\n",
    "tb = bdata.sample(frac=0.8, random_state=1)\n",
    "vb = bdata.drop(tb.index)\n",
    "tc = cdata_lim.sample(frac=0.8, random_state=1)\n",
    "vc = cdata_left.sample(n=200, random_state=1)\n",
    "tk = kdata_lim.sample(frac=0.8, random_state=1)\n",
    "vk = kdata_left.sample(n=200, random_state=1)\n",
    "\n",
    "train1 = pd.concat([tb, tc, tk])\n",
    "valid1 = pd.concat([vb, vc, vk])\n",
    "all1 = pd.concat([bdata, cdata_lim, kdata_lim])\n",
    "\n",
    "\n",
    "## Option 2 - different sample\n",
    "# Limit Calvin & Kim to random 1000\n",
    "cdata_lim = cdata.sample(n=1000, random_state=2)\n",
    "cdata_left = cdata.drop(cdata_lim.index)\n",
    "kdata_lim = kdata.sample(n=1000, random_state=2)\n",
    "kdata_left = kdata.drop(kdata_lim.index)\n",
    "\n",
    "# Split & Combine\n",
    "tb = bdata.sample(frac=0.8, random_state=2)\n",
    "vb = bdata.drop(tb.index)\n",
    "tc = cdata_lim.sample(frac=0.8, random_state=2)\n",
    "vc = cdata_left.sample(n=200, random_state=2)\n",
    "tk = kdata_lim.sample(frac=0.8, random_state=2)\n",
    "vk = kdata_left.sample(n=200, random_state=2)\n",
    "\n",
    "train2 = pd.concat([tb, tc, tk])\n",
    "valid2 = pd.concat([vb, vc, vk])\n",
    "all2 = pd.concat([bdata, cdata_lim, kdata_lim])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Current set\n",
    "train = train2\n",
    "valid = valid2\n",
    "alldat =  all2\n",
    "\n",
    "# Shuffle\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "valid = valid.sample(frac=1).reset_index(drop=True)\n",
    "alldat = alldat.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(alldat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['wouldnt', 'wont', 'werent', 'wasnt', 'shouldnt', 'neednt', 'isnt', 'havent', 'hasnt', 'hadnt', 'ive','doesnt', 'didnt', 'couldnt', 'arent', 'aint', 'amp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~’–“”'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "punctuation = string.punctuation + '’' + '–' + '“' + '”'\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text clean/transform function\n",
    "\n",
    "def textonly(text):\n",
    "    # Remove URLS\n",
    "    text = re.sub('https?:\\/\\/.*', '', text)\n",
    "    # Remove Punctuation\n",
    "    text = re.sub('—', ' ', text)\n",
    "    text  = \"\".join([char for char in text if char not in punctuation])\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove Stopwords, Tokenize\n",
    "    return [word for word in nltk.word_tokenize(text) if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important bag of words features from naive model\n",
    "obamaimpfeat = ['americans', 'make', 'today', 'people', 'fair', 'vote', 'us', 'hearing', 'doyourjob', 'climate', 'supreme', 'change', 'court', 'garland', 'judge', 'rt', 'leaders', 'senate', 'obama', 'president']\n",
    "calvinimpfeat = ['still', 'happy', 'know', 'literally', 'love', 'stevemorris', 'yall', 'good', 'legend', 'omg', 'trump', 'really', 'one', 'dont', 'gay', 'people', 'like', 'yashar', 'im', 'rt']\n",
    "kimimpfeat = ['day', 'classic', '✨', 'kkwxmario', 'available', 'love', 'lip', 'kuwtk', 'today', '12pm', 'collection', 'get', 'happy', 'kimkardashian', 'kkwfragrance', 'pst', 'new', 'shop', 'kkwbeauty', 'rt']\n",
    "\n",
    "wordfeats = list(set(obamaimpfeat + calvinimpfeat + kimimpfeat))\n",
    "len(wordfeats)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kelsey\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# pd.options.display.max_colwidth = 500\n",
    "# # Dataframe with features\n",
    "# import numpy as np\n",
    "\n",
    "# X = alldat[['text', 'retweet_count']]\n",
    "\n",
    "# X['text_bascln'] = X['text'].apply(lambda x: textonly(x))\n",
    "\n",
    "# for w in wordfeats:\n",
    "#     #X[w] = 0\n",
    "#     print(w)\n",
    "#     print(X['text_bascln'])\n",
    "#     print(w in X['text_bascln'])\n",
    "#     X[w] = np.where(w in X['text_bascln'], 1, 0)\n",
    "    \n",
    "# X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode label\n",
    "y = alldat['handle']\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)\n",
    "# alpha order - 0=barack, 1 = calvin, 2=kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12pm', 'americans', 'available', 'change', 'classic', 'climate', 'collection', 'court', 'day', 'dont', 'doyourjob', 'fair', 'garland', 'gay', 'get', 'good', 'happy', 'hearing', 'im', 'judge', 'kimkardashian', 'kkwbeauty', 'kkwfragrance', 'kkwxmario', 'know', 'kuwtk', 'leaders', 'legend', 'like', 'lip', 'literally', 'love', 'make', 'new', 'obama', 'omg', 'one', 'people', 'president', 'pst', 'really', 'rt', 'senate', 'shop', 'stevemorris', 'still', 'supreme', 'today', 'trump', 'us', 'vote', 'yall', 'yashar']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(wordfeats)\n",
    "print(vectorizer.get_feature_names())\n",
    "# ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "# >>> print(X.toarray())  \n",
    "# [[0 1 1 1 0 0 1 0 1]\n",
    "#  [0 2 0 1 0 1 1 0 1]\n",
    "#  [1 0 0 1 1 0 1 1 1]\n",
    "#  [0 1 1 1 0 0 1 0 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = alldat['text']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the bag-of-words transformer on the text-processed corpus # i.e., text_process() declared in II is executed...\n",
    "#bow_transformer=CountVectorizer(analyzer=textonly).fit(X_train)\n",
    "bow_transformer=CountVectorizer(analyzer=textonly).fit(wordfeats)\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_train=bow_transformer.transform(X_train)#ONLY TRAINING DATA\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_test=bow_transformer.transform(X_test)#TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# instantiating the model with Multinomial Naive Bayes..\n",
    "model = MultinomialNB()\n",
    "# training the model...\n",
    "model = model.fit(text_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58875"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training accuracy\n",
    "model.score(text_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x54 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testcase = \"Don’t make time for people who don’t make time for you. https://t.co/5iSinrlCz1\"\n",
    "bow_transformer.transform([testcase])\n",
    "#print(loaded_model.predict_proba(bow_transformer.transform([testcase]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12pm',\n",
       " 'americans',\n",
       " 'available',\n",
       " 'change',\n",
       " 'classic',\n",
       " 'climate',\n",
       " 'collection',\n",
       " 'court',\n",
       " 'day',\n",
       " 'dont',\n",
       " 'doyourjob',\n",
       " 'fair',\n",
       " 'garland',\n",
       " 'gay',\n",
       " 'get',\n",
       " 'good',\n",
       " 'happy',\n",
       " 'hearing',\n",
       " 'im',\n",
       " 'judge',\n",
       " 'kimkardashian',\n",
       " 'kkwbeauty',\n",
       " 'kkwfragrance',\n",
       " 'kkwxmario',\n",
       " 'know',\n",
       " 'kuwtk',\n",
       " 'leaders',\n",
       " 'legend',\n",
       " 'like',\n",
       " 'lip',\n",
       " 'literally',\n",
       " 'love',\n",
       " 'make',\n",
       " 'new',\n",
       " 'obama',\n",
       " 'omg',\n",
       " 'one',\n",
       " 'people',\n",
       " 'president',\n",
       " 'pst',\n",
       " 'really',\n",
       " 'rt',\n",
       " 'senate',\n",
       " 'shop',\n",
       " 'stevemorris',\n",
       " 'still',\n",
       " 'supreme',\n",
       " 'today',\n",
       " 'trump',\n",
       " 'us',\n",
       " 'vote',\n",
       " 'yall',\n",
       " 'yashar',\n",
       " '✨']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open(basepath + 'words.pkl', 'wb') as f:\n",
    "    pickle.dump(bow_transformer.get_feature_names(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12pm',\n",
       " 'americans',\n",
       " 'available',\n",
       " 'change',\n",
       " 'classic',\n",
       " 'climate',\n",
       " 'collection',\n",
       " 'court',\n",
       " 'day',\n",
       " 'dont',\n",
       " 'doyourjob',\n",
       " 'fair',\n",
       " 'garland',\n",
       " 'gay',\n",
       " 'get',\n",
       " 'good',\n",
       " 'happy',\n",
       " 'hearing',\n",
       " 'im',\n",
       " 'judge',\n",
       " 'kimkardashian',\n",
       " 'kkwbeauty',\n",
       " 'kkwfragrance',\n",
       " 'kkwxmario',\n",
       " 'know',\n",
       " 'kuwtk',\n",
       " 'leaders',\n",
       " 'legend',\n",
       " 'like',\n",
       " 'lip',\n",
       " 'literally',\n",
       " 'love',\n",
       " 'make',\n",
       " 'new',\n",
       " 'obama',\n",
       " 'omg',\n",
       " 'one',\n",
       " 'people',\n",
       " 'president',\n",
       " 'pst',\n",
       " 'really',\n",
       " 'rt',\n",
       " 'senate',\n",
       " 'shop',\n",
       " 'stevemorris',\n",
       " 'still',\n",
       " 'supreme',\n",
       " 'today',\n",
       " 'trump',\n",
       " 'us',\n",
       " 'vote',\n",
       " 'yall',\n",
       " 'yashar',\n",
       " '✨']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(basepath + 'words.pkl', 'rb') as f:\n",
    "    mynewlist = pickle.load(f)\n",
    "\n",
    "mynewlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t1\n",
      "  (1, 43)\t1\n",
      "  (2, 37)\t1\n",
      "  (3, 50)\t1\n",
      "  (4, 22)\t1\n",
      "  (5, 45)\t1\n",
      "  (6, 15)\t1\n",
      "  (7, 30)\t1\n",
      "  (9, 40)\t1\n",
      "  (10, 13)\t1\n",
      "  (11, 46)\t1\n",
      "  (12, 17)\t1\n",
      "  (13, 39)\t1\n",
      "  (14, 36)\t1\n",
      "  (15, 44)\t1\n",
      "  (16, 18)\t1\n",
      "  (17, 42)\t1\n",
      "  (18, 1)\t1\n",
      "  (19, 11)\t1\n",
      "  (20, 24)\t1\n",
      "  (21, 14)\t1\n",
      "  (22, 0)\t1\n",
      "  (23, 19)\t1\n",
      "  (24, 23)\t1\n",
      "  (25, 25)\t1\n",
      "  :\t:\n",
      "  (29, 49)\t1\n",
      "  (30, 41)\t1\n",
      "  (31, 48)\t1\n",
      "  (32, 6)\t1\n",
      "  (33, 3)\t1\n",
      "  (34, 33)\t1\n",
      "  (35, 26)\t1\n",
      "  (36, 20)\t1\n",
      "  (37, 5)\t1\n",
      "  (38, 29)\t1\n",
      "  (39, 4)\t1\n",
      "  (40, 8)\t1\n",
      "  (41, 51)\t1\n",
      "  (42, 32)\t1\n",
      "  (43, 12)\t1\n",
      "  (44, 7)\t1\n",
      "  (45, 47)\t1\n",
      "  (46, 16)\t1\n",
      "  (47, 9)\t1\n",
      "  (48, 27)\t1\n",
      "  (49, 38)\t1\n",
      "  (50, 34)\t1\n",
      "  (51, 52)\t1\n",
      "  (52, 21)\t1\n",
      "  (53, 31)\t1\n"
     ]
    }
   ],
   "source": [
    "# Create features\n",
    "\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "train['text_bascln'] = train['text'].apply(lambda x: textonly(x))\n",
    "train[['text', 'text_bascln']].head(10)\n",
    "\n",
    "\n",
    "df.drop('Col3', 1).join(df.Col3.str.join('|').str.get_dummies())\n",
    "X_train.join(df.Col3.str.join('|').str.get_dummies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
